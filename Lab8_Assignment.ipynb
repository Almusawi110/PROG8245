{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab8 Assignment Task PROG8245 - NLP Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name: Fatimah Almusawi\n",
    "### ID: 9000400"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the discussed topic in class for tokenizers, stop-word removal, stemming/lemmatization, and POS Tagging. <br><br>\n",
    "Create **ONE** function, that takes as an input a string, and returns the output of a string after stemming/lemmatization.<br><br>\n",
    "**Kindly note that you are required to consider the POS Tag while doing your stemming or lemmatization step (you should use whatever is more suitable for this task)** <br><br>\n",
    "After creating the function, you need to run your function on 10 **Random** files from reuters corpus, an example of how to download and load a file of reuters corpus is below. <br><br>\n",
    "**Your 10 **Random** files should be retrieved by getting a random array of length 10 which picks numbers RANDOMLY from 0 to len(reuters.fileids()), then the elements retrieved will be your corpus.<br> <br>*You need to set your Seed to be Equal to the last 3 digits in your studentID.*<br>** If your ID is 8000888 then seed =888 <br>\n",
    "**You may need to tailor your task based on the dataset to remove some special characters.**\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Last Step\n",
    "After finishing your code, run your code and save the result in a python dictionary, which would be of format:<br>\n",
    "{DocumentID: [List of Words], <br>\n",
    "...} <br>\n",
    "Save your python dictionary as a JSON file, or Pickle file. <br>\n",
    "A sample code for saving a python dictionary is available at the end of this notebook.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Import necessary libraries and download NLTK data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\marya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\marya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\marya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\marya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\marya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10788"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import nltk\n",
    "from nltk.corpus import reuters, stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "import random\n",
    "\n",
    "# Download necessary NLTK data\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "nltk.download('reuters') #downloading reuters corpus\n",
    "len(reuters.fileids()) #checking how many files are there"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Stemming Or lemmatization <br>\n",
    "I believe that lemmatization is a better option than stemming. This is because when it comes to understanding the meaning and context of words, lemmatization takes into account the complete word and how it is used to convert it back to its base or dictionary form. Unlike stemming, which simply cuts off word endings and sometimes results in meaningless words, lemmatization ensures that the modified word is a real and meaningful one. This makes lemmatization especially useful for tasks that require a high level of linguistic accuracy, such as analyzing the sentiment of a text or categorizing content by topic. By considering the part of speech and using a comprehensive dictionary, lemmatization delivers a more precise and valuable outcome for these types of tasks.\n",
    "\n",
    "source: https://www.analyticsvidhya.com/blog/2022/06/stemming-vs-lemmatization-in-nlp-must-know-differences/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"Converts treebank tags to wordnet tags.\"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # Default case\n",
    "\n",
    "def nlp_preprocess(sample_paragraph):\n",
    "    # Tokenize the paragraph\n",
    "    tokens = word_tokenize(sample_paragraph)\n",
    "    \n",
    "    # Lowercase and remove non-alphabetic characters\n",
    "    tokens = [token.lower() for token in tokens if token.isalpha()]\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # POS tagging\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token, get_wordnet_pos(tag)) for token, tag in tagged_tokens]\n",
    "    \n",
    "    return lemmatized_tokens\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of the `get_wordnet_pos` is to format the tagset to a with what is used with `WordNet`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Set Seed and Select 10 Random Files<br>\n",
    "here I will set my Seed to be Equal to the last 3 digits in my studentID.*<br>** so my ID is 9000400 then seed =400<br>\n",
    "### 4.saving the result in a python dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'training/1331': ['api', 'say', 'distillate', 'stock', 'mln', 'bbls', 'gasoline', 'crude', 'api', 'say', 'distillate', 'stock', 'mln', 'bbls', 'gasoline', 'crude'], 'training/7543': ['korean', 'february', 'current', 'account', 'surplus', 'narrow', 'south', 'korea', 'current', 'account', 'surplus', 'narrow', 'mln', 'dlrs', 'february', 'mln', 'january', 'compare', 'deficit', 'mln', 'dlrs', 'february', 'last', 'year', 'provisional', 'bank', 'korea', 'figure', 'show', 'current', 'account', 'two', 'month', 'january', 'february', 'swung', 'surplus', 'billion', 'dlrs', 'deficit', 'mln', 'dlrs', 'period', 'february', 'trade', 'surplus', 'narrow', 'mln', 'dlrs', 'mln', 'january', 'compare', 'deficit', 'mln', 'dlrs', 'year', 'ago', 'overall', 'balance', 'payment', 'surplus', 'rise', 'mln', 'dlrs', 'february', 'mln', 'january', 'mln', 'february', 'export', 'billion', 'dlrs', 'february', 'billion', 'january', 'billion', 'february', 'last', 'year', 'import', 'billion', 'billion', 'billion', 'february', 'invisible', 'trade', 'surplus', 'rise', 'mln', 'dlrs', 'mln', 'january', 'compare', 'deficit', 'mln', 'year', 'ago', 'transfer', 'payment', 'surplus', 'widened', 'mln', 'dlrs', 'february', 'mln', 'january', 'mln', 'year', 'ago', 'capital', 'account', 'surplus', 'mln', 'dlrs', 'february', 'mln', 'january', 'mln', 'february', 'last', 'year', 'capital', 'account', 'surplus', 'mln', 'dlrs', 'mln', 'mln', 'error', 'omission', 'account', 'left', 'surplus', 'mln', 'dlrs', 'february', 'deficit', 'mln', 'january', 'mln', 'february'], 'training/11764': ['bank', 'japan', 'buy', 'dollar', 'around', 'yen', 'dealer', 'say', 'bank', 'japan', 'buy', 'dollar', 'around', 'yen', 'dealer', 'say'], 'test/18111': ['stewart', 'lt', 'sell', 'plant', 'sara', 'lee', 'lt', 'sle', 'stewart', 'sandwich', 'inc', 'say', 'sell', 'coffee', 'roast', 'plant', 'sara', 'lee', 'corp', 'superior', 'coffee', 'food', 'subsidiary', 'undisclosed', 'term', 'company', 'say', 'superior', 'become', 'exclusive', 'packer', 'squire', 'label', 'coffee', 'product', 'market', 'stewart', 'stewart', 'provide', 'equipment', 'service', 'distribution', 'suppoort', 'superior', 'coffee', 'account'], 'training/7639': ['canada', 'northwest', 'sell', 'preferred', 'share', 'lt', 'canada', 'northwest', 'energy', 'ltd', 'say', 'reach', 'agreement', 'principle', 'sell', 'one', 'mln', 'prefer', 'share', 'way', 'private', 'placement', 'raise', 'mln', 'dlrs', 'oil', 'gas', 'exploration', 'pct', 'cumulative', 'redeemable', 'convertible', 'series', 'prefer', 'share', 'share', 'entitle', 'holder', 'tax', 'deduction', 'claim', 'company', 'share', 'convertible', 'anytime', 'issue', 'common', 'share', 'equivalent', 'conversion', 'price', 'dlrs', 'share', 'one', 'year', 'redeemable', 'time', 'company', 'dlrs', 'share', 'plus', 'accrued', 'dividend'], 'training/5654': ['telequest', 'inc', 'lt', 'telq', 'qtr', 'dec', 'loss', 'shr', 'loss', 'nine', 'ct', 'vs', 'profit', 'ct', 'net', 'loss', 'v', 'profit', 'sale', 'vs', 'avg', 'shrs', 'v', 'year', 'shr', 'profit', 'one', 'ct', 'v', 'profit', 'ct', 'net', 'profit', 'v', 'profit', 'sale', 'mln', 'vs', 'mln', 'avg', 'shrs', 'v'], 'training/4910': ['sunter', 'begin', 'tender', 'allegheny', 'lt', 'ag', 'allegheny', 'international', 'inc', 'say', 'sunter', 'acquisititon', 'corp', 'form', 'first', 'boston', 'inc', 'begin', 'cash', 'tender', 'offer', 'accordance', 'previously', 'announce', 'merger', 'agreement', 'outsanding', 'share', 'common', 'stock', 'dlr', 'cumulative', 'preference', 'stock', 'dlrs', 'convertible', 'preferred', 'stock', 'offer', 'expire', 'april', 'unless', 'extend', 'term', 'agreement', 'tender', 'shareholder', 'receive', 'dlrs', 'per', 'common', 'share', 'dlrs', 'per', 'preference', 'share', 'dlrs', 'per', 'prefer', 'share', 'allegheny', 'say', 'offer', 'condition', 'upon', 'valid', 'tender', 'stock', 'represent', 'majority', 'vote', 'power', 'least', 'outstanding', 'share', 'preference', 'stock', 'prefer', 'stock', 'merger', 'agreement', 'announce', 'early', 'week'], 'training/3711': ['justice', 'department', 'review', 'icahn', 'usair', 'filing', 'department', 'justice', 'doj', 'review', 'whether', 'trans', 'world', 'airline', 'inc', 'lt', 'twa', 'chairman', 'carl', 'icahn', 'violate', 'federal', 'antitrust', 'law', 'fail', 'seek', 'advance', 'clearance', 'doj', 'federal', 'trade', 'commission', 'extensive', 'purchase', 'usair', 'group', 'inc', 'lt', 'u', 'stock', 'doj', 'official', 'tell', 'senate', 'panel', 'matter', 'look', 'charles', 'rule', 'act', 'assistant', 'attorney', 'tell', 'senate', 'judiciary', 'committee', 'antitrust', 'subcommittee', 'rule', 'decline', 'comment', 'review', 'continue', 'rule', 'respond', 'panel', 'chairman', 'howard', 'metzenbaum', 'ask', 'department', 'act', 'appear', 'clear', 'violation', 'law', 'metzenbaum', 'say', 'icahn', 'fail', 'file', 'notification', 'form', 'ftc', 'prior', 'purchase', 'mln', 'dlrs', 'worth', 'usair', 'stock', 'rule', 'say', 'airline', 'company', 'purchaser', 'would', 'exempt', 'requirement', 'instead', 'file', 'merger', 'application', 'department', 'transportation', 'icahn', 'file', 'application', 'dot', 'file', 'thrown', 'dot', 'friday', 'dot', 'throw', 'application', 'late', 'friday', 'ground', 'lack', 'necessary', 'data', 'government', 'review', 'propose', 'usair', 'takeover', 'bid', 'icahn', 'refiled', 'complete', 'application', 'form', 'monday', 'rule', 'pledge', 'act', 'icahn', 'twa', 'violation', 'find'], 'training/401': ['miller', 'tabak', 'pct', 'penn', 'traffic', 'lt', 'pnf', 'lt', 'miller', 'tabak', 'hirsch', 'co', 'say', 'receive', 'accepted', 'common', 'share', 'penn', 'traffic', 'co', 'response', 'dlr', 'per', 'share', 'tender', 'offer', 'expire', 'friday', 'together', 'share', 'already', 'own', 'pct', 'penn', 'traffic', 'company', 'say', 'penn', 'traffic', 'expect', 'hold', 'special', 'shareholder', 'meeting', 'later', 'month', 'approve', 'merger', 'miller', 'tabak', 'tender', 'price', 'say', 'two', 'miller', 'tabak', 'representative', 'name', 'penn', 'traffic', 'board', 'march', 'four', 'serve', 'director', 'penn', 'traffic', 'president', 'chief', 'executive', 'officer', 'guido', 'malacarne', 'company', 'say', 'received', 'financing', 'transaction', 'first', 'national', 'bank', 'minneapolis', 'salomon', 'inc', 'lt', 'sb'], 'training/7129': ['canada', 'retail', 'sale', 'fall', 'pct', 'january', 'canada', 'retail', 'sale', 'seasonally', 'adjust', 'fell', 'pct', 'january', 'gain', 'pct', 'december', 'statistic', 'canada', 'say', 'retail', 'sale', 'fell', 'billion', 'dlrs', 'billion', 'dlrs', 'december', 'unadjusted', 'sale', 'pct', 'high', 'january', 'january', 'automobile', 'sale', 'fell', 'pct', 'department', 'store', 'sale', 'slip', 'pct', 'variety', 'store', 'sale', 'plunge', 'pct', 'decline', 'offset', 'pct', 'increase', 'grocery', 'store', 'sale', 'pct', 'gain', 'hardware', 'sale']}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# Set the seed for reproducibility\n",
    "random.seed(400)\n",
    "\n",
    "# Select 10 random document IDs from the Reuters corpus\n",
    "document_ids = random.sample(reuters.fileids(), 10)\n",
    "\n",
    "# Initialize a dictionary to store the processed text for each document\n",
    "processed_texts = {}\n",
    "\n",
    "# Process each document and store the result in the dictionary\n",
    "for doc_id in document_ids:\n",
    "    text = reuters.raw(doc_id)\n",
    "    processed_text = nlp_preprocess(text)\n",
    "    processed_texts[doc_id] = processed_text\n",
    "\n",
    "# Save the dictionary to a file using Pickle\n",
    "with open('output.pkl', 'wb') as file:\n",
    "    pickle.dump(processed_texts, file)\n",
    "\n",
    "# To read the pkl file and print the dictionary\n",
    "with open('output.pkl', 'rb') as file:\n",
    "    loaded_dict = pickle.load(file)\n",
    "\n",
    "# Print the loaded dictionary to verify\n",
    "print(loaded_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:<br>\n",
    "In this notebook, we have explored the process of text preprocessing for natural language processing (NLP) tasks. We have focused on several techniques, including tokenization, stop-word removal, part-of-speech (POS) tagging, and lemmatization, to prepare the text data for analysis. \n",
    "\n",
    "To simplify the process, we have created a single function that performs all these steps. This function takes into account the POS tags to ensure accurate lemmatization. We have applied this function to 10 randomly selected documents from the Reuters corpus, and the results have been stored in a Python dictionary and saved as a Pickle file. \n",
    "\n",
    "We have chosen lemmatization over stemming for its higher linguistic accuracy and context understanding. This is essential for detailed language analysis tasks that require precise meaning and context."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSCN8010_classic_ml",
   "language": "python",
   "name": "cscn8010_classic_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
