{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 11: Vector Space Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will walk through a simple example of Vector Space Modelling. Then we will use cosine similarity to find similarity between document and query and rank the documents accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_docs = ['The quick brown fox jumps over the lazy dog.',\n",
    "               'A brown dog chased the fox.',\n",
    "               'The dog is lazy.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.'],\n",
       " ['A', 'brown', 'dog', 'chased', 'the', 'fox', '.'],\n",
       " ['The', 'dog', 'is', 'lazy', '.']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## First step is to tokenize our text\n",
    "tokenized_documents = [word_tokenize(document) for document in sample_docs]\n",
    "tokenized_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['quick', 'brown', 'fox', 'jumps', 'lazy', 'dog', '.'],\n",
       " ['brown', 'dog', 'chased', 'fox', '.'],\n",
       " ['dog', 'lazy', '.']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Second step is to calculate our TF-IDF \n",
    "## We need first to preprocess our text\n",
    "## For simplicity I will just remove the stop words in documents\n",
    "## and I will change words to lower\n",
    "from nltk.corpus import stopwords\n",
    "english_stopwords = stopwords.words('english')\n",
    "cleaned_data = [[word.lower() for word in document if word.lower() not in english_stopwords] for document in tokenized_documents]\n",
    "cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['quick brown fox jumps lazy dog .', 'brown dog chased fox .', 'dog lazy .']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## TF_IDF vectorizer takes as an input sentences, lets join our tokens\n",
    "cleaned_sentences = [' '.join(document) for document in cleaned_data]\n",
    "cleaned_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t0.29225439586501756\n",
      "  (0, 5)\t0.37633074615060896\n",
      "  (0, 4)\t0.49482970636510465\n",
      "  (0, 3)\t0.37633074615060896\n",
      "  (0, 0)\t0.37633074615060896\n",
      "  (0, 6)\t0.49482970636510465\n",
      "  (1, 1)\t0.6317450542765208\n",
      "  (1, 2)\t0.3731188059313277\n",
      "  (1, 3)\t0.4804583972923858\n",
      "  (1, 0)\t0.4804583972923858\n",
      "  (2, 2)\t0.6133555370249717\n",
      "  (2, 5)\t0.7898069290660905 {'quick': 6, 'brown': 0, 'fox': 3, 'jumps': 4, 'lazy': 5, 'dog': 2, 'chased': 1}\n"
     ]
    }
   ],
   "source": [
    "## Lets define our vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(cleaned_sentences)\n",
    "print(tfidf_matrix,tfidf_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'brown', 'dog', 'cat']\n",
      "['brown', 'dog', 'cat']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['brown dog cat']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Given that we have the TFIDF vectors, lets write the query and then get the vector of the query\n",
    "query = \"the brown dog cat\"\n",
    "## Preprocess the query\n",
    "query_tokens = word_tokenize(query)\n",
    "print(query_tokens)\n",
    "query_cleaned = [word.lower() for word in query_tokens if word.lower() not in english_stopwords]\n",
    "print(query_cleaned)\n",
    "query_cleaned_combined = [' '.join(query_cleaned)]\n",
    "\n",
    "query_cleaned_combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t0.6133555370249717\n",
      "  (0, 0)\t0.7898069290660905\n"
     ]
    }
   ],
   "source": [
    "## Get the TFIDF vector of the query\n",
    "query_tfIdf_vector = tfidf_vectorizer.transform(query_cleaned_combined)\n",
    "print(query_tfIdf_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.47648448, 0.60832386, 0.37620501]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Now we need to find the cosine similarity between the query and documents\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cosine_similarities = cosine_similarity(query_tfIdf_vector, tfidf_matrix)\n",
    "cosine_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The quick brown fox jumps over the lazy dog.', 0.4764844828540594),\n",
       " ('A brown dog chased the fox.', 0.6083238568956406),\n",
       " ('The dog is lazy.', 0.37620501479919144)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## To rank the documents, first we will create a list of ranked results\n",
    "results = [(sample_docs[i], cosine_similarities[0][i]) for i in range(len(sample_docs))]\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A brown dog chased the fox.', 0.6083238568956406),\n",
       " ('The quick brown fox jumps over the lazy dog.', 0.4764844828540594),\n",
       " ('The dog is lazy.', 0.37620501479919144)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Sorting the results based on similarity to rank the documents\n",
    "results.sort(key=lambda x:x[1], reverse=True)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands On Exercise InClass:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the list of the following relevant and retrieved documents. Find the precision and recall of this retrieval system.<br>\n",
    "Assume that we only have the documents that we can see in relevant or retrieved documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " precision: 0.4 recall: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Sample relevant documents and retrieved documents\n",
    "relevant_documents = [0, 1, 2, 4]\n",
    "retrieved_documents = [0, 1, 3, 5, 7]\n",
    "TP=len(set(relevant_documents) & set(retrieved_documents))\n",
    "precision=TP/len(retrieved_documents)\n",
    "recall=TP/len(relevant_documents)\n",
    "print(f' precision: {precision} recall: {recall}')\n",
    "\n",
    "    # for doc_rel in relevant_documents:\n",
    "    # if doc_rel in retrieved_documents:\n",
    "    #     TP += 1 \n",
    "    #     print(TP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate precision at k for the following k_values\n",
    "k_values = [1,3,5]\n",
    "\n",
    "# Calculate Precision at k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average precision\n",
    "\n",
    "# First, find the k values where you need to calculate the precision at\n",
    "\n",
    "\n",
    "average_precision_list = []\n",
    "# Calculate Precision at k\n",
    "\n",
    "\n",
    "# Calculate the average precision (average of precision at k's)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate Mean_Average_Precision Given that another IR System returns the following results\n",
    "retrieved_documents_ir2 = [0, 3, 7, 2]\n",
    "# Calculate the average precision\n",
    "\n",
    "# First, find the k values where you need to calculate the precision at\n",
    "\n",
    "average_precision_list_2 = []\n",
    "# Calculate Precision at k\n",
    "\n",
    "# Calculate the average precision (average of precision at k's)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Mean_Average_Precision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the list of the following documents and query. Find the cosine_similarity between documents and the query. Rank the documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"Natural language processing is a field of computer science.\",\n",
    "    \"Machine learning algorithms analyze data to make predictions.\",\n",
    "    \"Data preprocessing is essential for machine learning models.\",\n",
    "    \"Python is a popular programming language for data science.\",\n",
    "    \"Information retrieval involves finding relevant information in a collection.\",\n",
    "    \"Neural networks are used in deep learning models.\",\n",
    "    \"Statistical analysis helps in understanding data patterns.\",\n",
    "    \"Big data technologies handle large volumes of data.\",\n",
    "    \"Classification and regression are types of supervised learning.\",\n",
    "    \"Clustering algorithms group similar data points together.\"\n",
    "]\n",
    "\n",
    "# TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Queries\n",
    "queries = [\n",
    "    \"What is the importance of preprocessing in machine learning?\",\n",
    "    \"How do neural networks contribute to deep learning?\"\n",
    "]\n",
    "# Convert query to TF-IDF representation\n",
    "\n",
    "# Calculate cosine similarity between query and documents\n",
    "   \n",
    "# Output top documents\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_actual_ranks = [[1,8,4],\n",
    "                        [1,2,5]]\n",
    "## Calculate the MAP given the results you got from cosine similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kappa Measure Example Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annotator 1's relevance assessments\n",
    "annotator1 = [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]  # 1 indicates relevant, 0 indicates not relevant\n",
    "\n",
    "# Annotator 2's relevance assessments (with some disagreements)\n",
    "annotator2 = [1, 1, 0, 0, 1, 0, 1, 0, 1, 1]  # 1 indicates relevant, 0 indicates not relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Kappa Score is: 0.3999999999999999\n"
     ]
    }
   ],
   "source": [
    "## Calculate the Kappa Score of Confidence of the two annotations\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix_res = confusion_matrix(annotator2,annotator1)\n",
    "total_docs = len(annotator1)\n",
    "prob_agreeing = (confusion_matrix_res[0,0]+confusion_matrix_res[1,1])/total_docs\n",
    "prob_relevant = (sum(confusion_matrix_res[:,1])/total_docs) * \\\n",
    "                (sum(confusion_matrix_res[1,:])/total_docs) \n",
    "prob_not_relevant = (sum(confusion_matrix_res[:,0])/total_docs) * \\\n",
    "                (sum(confusion_matrix_res[0,:])/total_docs) \n",
    "prob_chance = prob_relevant+prob_not_relevant\n",
    "kappa_score = (prob_agreeing-prob_chance)/(1-prob_chance)\n",
    "print(f\"Your Kappa Score is: {kappa_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen's Kappa for IR: 0.4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "# Compute Cohen's Kappa for relevance assessments\n",
    "kappa_ir = cohen_kappa_score(annotator1, annotator2)\n",
    "\n",
    "print(f\"Cohen's Kappa for IR: {kappa_ir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This output suggests a fair agreement between the two annotators on the relevance assessments. More annotators are needed or replace one of the annotators."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSCN8010_classic_ml",
   "language": "python",
   "name": "cscn8010_classic_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
