{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Assignment 10: <br>\n",
    "### Student Name:Fatimah Almusawi\n",
    "### Student ID:900400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment targets to assure that the students understood basic IR concepts.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 1: Implementing an IR System (10/16)**<br>\n",
    "\n",
    "Consider a collection of 1000 documents and a set of 10 queries. Implement an IR system based on the Vector Space Model (VSM) using TF-IDF weighting.<br>\n",
    "\n",
    "Dataset:<br>\n",
    "\n",
    "- 1000 documents (text content for each document)\n",
    "- 10 sample queries <br>\n",
    "IR System:<br>\n",
    "\n",
    "Implement TF-IDF calculation for document-term matrix construction. <br>\n",
    "Develop a cosine similarity-based retrieval system to rank documents for each query. <br>\n",
    "Rank the top 10 documents for each query using the IR system. <br>\n",
    "Evaluation: <br>\n",
    "\n",
    "Compute Precision at k (P@k) for k=5, k=6, and k=10 for each query.<br>\n",
    "Calculate Mean Average Precision (MAP) across all queries. <br>\n",
    "Calculate the Mean Reciprocal Rank (MRR) across all queries.(3) <br><br>\n",
    "**Part 2: Assessing Inter-Annotator Agreement (6/16)**<br>\n",
    "\n",
    "Given the relevance assessments by three different annotators for a set of documents:<br>\n",
    "\n",
    "Annotator 1,2 and 3 relevance assessments<br>\n",
    "\n",
    "You are expected to: <br>\n",
    "\n",
    "Compute pairwise Cohen's Kappa values for the annotators' relevance assessments.<br>\n",
    "Discuss the observed agreement levels among annotators.<br>\n",
    "Explain how to improve the kappa value if we are not satisfied with the kappa.<br>\n",
    "Hint: Check Kappa Measure Last Slide Week 11\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------\n",
    "- Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "documents = [\n",
    "    \"Machine learning is transforming various industries.\",\n",
    "    \"Natural language processing helps in text analysis.\",\n",
    "    \"AI algorithms can improve decision-making processes.\",\n",
    "    \"Data science involves extracting insights from data.\",\n",
    "    \"Robotics is a field combining hardware and software.\",\n",
    "    \"Deep learning models require large amounts of data.\",\n",
    "    \"Blockchain technology secures digital transactions.\",\n",
    "    \"Cloud computing offers scalable computing power.\",\n",
    "    \"Virtual reality provides immersive experiences.\",\n",
    "    \"Augmented reality enhances real-world environments.\",\n",
    "    \"Internet of Things connects devices for data exchange.\",\n",
    "    \"Biometric authentication ensures secure access.\",\n",
    "    \"Quantum computing promises faster computations.\",\n",
    "    \"Cybersecurity protects against digital threats.\",\n",
    "    \"Ethical considerations in AI development are crucial.\",\n",
    "    \"Healthcare benefits from AI-driven diagnostics.\",\n",
    "    \"E-commerce relies on personalized recommendation systems.\",\n",
    "    \"Autonomous vehicles revolutionize transportation.\",\n",
    "    \"Smart cities utilize IoT for efficient operations.\",\n",
    "    \"Social media analysis aids in understanding trends.\",\n",
    "    \"Predictive analytics anticipates future outcomes.\",\n",
    "    \"Remote work trends increase reliance on digital tools.\",\n",
    "    \"Energy efficiency through smart grids is vital.\",\n",
    "    \"Fintech innovations reshape financial services.\",\n",
    "    \"Data privacy concerns arise in the era of big data.\",\n",
    "    \"Artificial general intelligence remains a challenge.\",\n",
    "    \"Human-computer interaction shapes user experiences.\",\n",
    "    \"Big data analytics drives informed decision-making.\",\n",
    "    \"Climate change predictions benefit from AI models.\",\n",
    "    \"Bioinformatics applies computational techniques to biology.\",\n",
    "    \"Robotic process automation streamlines workflows.\",\n",
    "    \"Industry 4.0 integrates AI with manufacturing.\",\n",
    "    \"Sentiment analysis detects emotions in text data.\",\n",
    "    \"Reinforcement learning powers autonomous agents.\",\n",
    "    \"Data visualization simplifies complex information.\",\n",
    "    \"Edge computing reduces latency in data processing.\",\n",
    "    \"Smart farming optimizes agricultural practices.\",\n",
    "    \"AI in education enhances personalized learning.\",\n",
    "    \"Natural disaster predictions leverage machine learning.\",\n",
    "    \"Media recommendation systems personalize content.\",\n",
    "    \"Speech recognition enables hands-free interactions.\",\n",
    "    \"Genetic algorithms mimic natural selection.\",\n",
    "    \"Neural networks simulate the human brain.\",\n",
    "    \"Behavioral analytics uncovers patterns in behavior.\",\n",
    "    \"Spatial analysis benefits from geospatial data.\",\n",
    "    \"Explainable AI improves transparency in models.\",\n",
    "    \"AI ethics guide responsible technology development.\",\n",
    "    \"Prescriptive analytics offers actionable insights.\",\n",
    "    \"Supply chain optimization employs predictive models.\",\n",
    "    \"Emotion AI detects emotions in facial expressions.\",\n",
    "    \"Distributed ledger technology ensures secure transactions.\",\n",
    "    \"Biological data analysis aids in medical research.\",\n",
    "    \"Smart grid technology enhances energy distribution.\",\n",
    "    \"AI-powered chatbots automate customer support.\",\n",
    "    \"Machine translation breaks language barriers.\",\n",
    "    \"Personalized medicine tailors treatments to individuals.\",\n",
    "    \"Data-driven marketing optimizes customer engagement.\",\n",
    "    \"Smart home devices enhance living experiences.\",\n",
    "    \"Time series forecasting predicts future trends.\",\n",
    "    \"AI-driven creativity challenges human capabilities.\",\n",
    "    \"Intelligent document processing automates data extraction.\",\n",
    "    \"Graph analytics explores relationships in networks.\",\n",
    "    \"Adversarial attacks pose challenges to AI security.\",\n",
    "    \"Predictive maintenance minimizes equipment downtime.\",\n",
    "    \"Spatial reasoning enhances AI navigation systems.\",\n",
    "    \"Privacy-preserving techniques protect sensitive data.\",\n",
    "    \"Automated decision-making raises ethical concerns.\",\n",
    "    \"Behavioral biometrics verifies user identities.\",\n",
    "    \"Explainable recommendations increase user trust.\",\n",
    "    \"Quantum machine learning explores quantum states.\",\n",
    "    \"Internet censorship detection uses AI techniques.\",\n",
    "    \"Smart wearables monitor health and fitness.\",\n",
    "    \"Intelligent tutoring systems adapt to student needs.\",\n",
    "    \"Graph neural networks model complex relationships.\",\n",
    "    \"AI in sports analytics improves performance analysis.\",\n",
    "    \"Facial recognition technology raises privacy debates.\",\n",
    "    \"Digital twin technology replicates physical systems.\",\n",
    "    \"Emotion detection in video content aids analysis.\",\n",
    "    \"Neuromorphic computing mimics brain structure.\",\n",
    "    \"Robotic surgery enhances precision in operations.\",\n",
    "    \"AI-powered language translation assists global communication.\",\n",
    "    \"Automated content moderation filters online content.\",\n",
    "    \"Network anomaly detection identifies security threats.\",\n",
    "    \"Predictive policing uses data to prevent crime.\",\n",
    "    \"Quantum cryptography ensures secure communications.\",\n",
    "    \"AI-generated art challenges traditional creativity.\",\n",
    "    \"Ethical considerations in autonomous vehicles are debated.\",\n",
    "    \"Adaptive learning systems personalize educational content.\",\n",
    "    \"Biomedical image analysis aids in disease diagnosis.\",\n",
    "    \"Explainable computer vision interprets image features.\",\n",
    "    \"AI-driven personalization enhances user experiences.\",\n",
    "    \"Conversational AI improves human-like interactions.\",\n",
    "    \"Federated learning protects individual data privacy.\",\n",
    "    \"Human-centered AI design prioritizes user needs.\",\n",
    "    \"Quantum annealing solves optimization problems.\",\n",
    "    \"Behavior-based authentication enhances security.\",\n",
    "    \"AI in music composition transforms creative processes.\",\n",
    "    \"Semantic search improves information retrieval.\",\n",
    "    \"Recommender systems optimize user preferences.\",\n",
    "    \"Information Retrieval systems are outdated without neural networks\"\n",
    "]\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "queries = [\n",
    "    \"What are the impacts of AI on healthcare?\",\n",
    "    \"How does machine learning improve financial services?\",\n",
    "    \"What is the role of AI in autonomous vehicles?\",\n",
    "    \"Explain the applications of natural language processing.\",\n",
    "    \"How does robotics benefit from AI integration?\",\n",
    "    \"What are the challenges in implementing AI in education?\",\n",
    "    \"How does data science contribute to climate change predictions?\",\n",
    "    \"What are the ethical considerations in AI development?\",\n",
    "    \"Explain the significance of AI in smart cities.\",\n",
    "    \"How does AI enhance cybersecurity measures?\"\n",
    "]\n",
    "print(len(queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "ground_truth =[[3, 74, 38, 92, 71, 72, 98, 48, 18, 100], \n",
    "                [12, 26, 52, 54, 21, 63, 64, 45, 14, 83],\n",
    "                [58, 77, 62, 31, 6, 61, 36, 96, 85, 18], \n",
    "                [99, 62, 98, 34, 63, 79, 43, 31, 3, 16],\n",
    "                [8, 88, 73, 82, 37, 25, 34, 87, 66, 58], \n",
    "                [90, 40, 84, 64, 34, 2, 73, 23, 59, 89],\n",
    "                [80, 10, 86, 21, 68, 37, 83, 57, 6, 98],\n",
    "                [22, 3, 2, 44, 56, 80, 50, 63, 87, 13],\n",
    "                [1, 18, 22, 44, 99, 72, 24, 95, 10, 87],\n",
    "                [31, 10, 56, 50, 75, 4, 18, 85, 84, 74]\n",
    "                           ]\n",
    "print(len(ground_truth))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Data Loading\n",
    "\n",
    "First, we load our dataset comprising 1000 documents. This dataset will serve as the basis for our Information Retrieval (IR) system. The documents are loaded into a list where each entry represents a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Implement your solution for part 1\n",
    "# Assuming documents are stored in a list called 'documents'\n",
    "documents = [\"Document 1 text\", \"Document 2 text\", ..., \"Document 1000 text\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 2: Tokenization and removing stopwords\n",
    "Here, we tokenize our documents, breaking them down into individual words. This step is crucial for analyzing the text at the word level.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['machine learning transforming various industries .',\n",
       " 'natural language processing helps text analysis .',\n",
       " 'ai algorithms improve decision-making processes .',\n",
       " 'data science involves extracting insights data .',\n",
       " 'robotics field combining hardware software .',\n",
       " 'deep learning models require large amounts data .',\n",
       " 'blockchain technology secures digital transactions .',\n",
       " 'cloud computing offers scalable computing power .',\n",
       " 'virtual reality provides immersive experiences .',\n",
       " 'augmented reality enhances real-world environments .',\n",
       " 'internet things connects devices data exchange .',\n",
       " 'biometric authentication ensures secure access .',\n",
       " 'quantum computing promises faster computations .',\n",
       " 'cybersecurity protects digital threats .',\n",
       " 'ethical considerations ai development crucial .',\n",
       " 'healthcare benefits ai-driven diagnostics .',\n",
       " 'e-commerce relies personalized recommendation systems .',\n",
       " 'autonomous vehicles revolutionize transportation .',\n",
       " 'smart cities utilize iot efficient operations .',\n",
       " 'social media analysis aids understanding trends .',\n",
       " 'predictive analytics anticipates future outcomes .',\n",
       " 'remote work trends increase reliance digital tools .',\n",
       " 'energy efficiency smart grids vital .',\n",
       " 'fintech innovations reshape financial services .',\n",
       " 'data privacy concerns arise era big data .',\n",
       " 'artificial general intelligence remains challenge .',\n",
       " 'human-computer interaction shapes user experiences .',\n",
       " 'big data analytics drives informed decision-making .',\n",
       " 'climate change predictions benefit ai models .',\n",
       " 'bioinformatics applies computational techniques biology .',\n",
       " 'robotic process automation streamlines workflows .',\n",
       " 'industry 4.0 integrates ai manufacturing .',\n",
       " 'sentiment analysis detects emotions text data .',\n",
       " 'reinforcement learning powers autonomous agents .',\n",
       " 'data visualization simplifies complex information .',\n",
       " 'edge computing reduces latency data processing .',\n",
       " 'smart farming optimizes agricultural practices .',\n",
       " 'ai education enhances personalized learning .',\n",
       " 'natural disaster predictions leverage machine learning .',\n",
       " 'media recommendation systems personalize content .',\n",
       " 'speech recognition enables hands-free interactions .',\n",
       " 'genetic algorithms mimic natural selection .',\n",
       " 'neural networks simulate human brain .',\n",
       " 'behavioral analytics uncovers patterns behavior .',\n",
       " 'spatial analysis benefits geospatial data .',\n",
       " 'explainable ai improves transparency models .',\n",
       " 'ai ethics guide responsible technology development .',\n",
       " 'prescriptive analytics offers actionable insights .',\n",
       " 'supply chain optimization employs predictive models .',\n",
       " 'emotion ai detects emotions facial expressions .',\n",
       " 'distributed ledger technology ensures secure transactions .',\n",
       " 'biological data analysis aids medical research .',\n",
       " 'smart grid technology enhances energy distribution .',\n",
       " 'ai-powered chatbots automate customer support .',\n",
       " 'machine translation breaks language barriers .',\n",
       " 'personalized medicine tailors treatments individuals .',\n",
       " 'data-driven marketing optimizes customer engagement .',\n",
       " 'smart home devices enhance living experiences .',\n",
       " 'time series forecasting predicts future trends .',\n",
       " 'ai-driven creativity challenges human capabilities .',\n",
       " 'intelligent document processing automates data extraction .',\n",
       " 'graph analytics explores relationships networks .',\n",
       " 'adversarial attacks pose challenges ai security .',\n",
       " 'predictive maintenance minimizes equipment downtime .',\n",
       " 'spatial reasoning enhances ai navigation systems .',\n",
       " 'privacy-preserving techniques protect sensitive data .',\n",
       " 'automated decision-making raises ethical concerns .',\n",
       " 'behavioral biometrics verifies user identities .',\n",
       " 'explainable recommendations increase user trust .',\n",
       " 'quantum machine learning explores quantum states .',\n",
       " 'internet censorship detection uses ai techniques .',\n",
       " 'smart wearables monitor health fitness .',\n",
       " 'intelligent tutoring systems adapt student needs .',\n",
       " 'graph neural networks model complex relationships .',\n",
       " 'ai sports analytics improves performance analysis .',\n",
       " 'facial recognition technology raises privacy debates .',\n",
       " 'digital twin technology replicates physical systems .',\n",
       " 'emotion detection video content aids analysis .',\n",
       " 'neuromorphic computing mimics brain structure .',\n",
       " 'robotic surgery enhances precision operations .',\n",
       " 'ai-powered language translation assists global communication .',\n",
       " 'automated content moderation filters online content .',\n",
       " 'network anomaly detection identifies security threats .',\n",
       " 'predictive policing uses data prevent crime .',\n",
       " 'quantum cryptography ensures secure communications .',\n",
       " 'ai-generated art challenges traditional creativity .',\n",
       " 'ethical considerations autonomous vehicles debated .',\n",
       " 'adaptive learning systems personalize educational content .',\n",
       " 'biomedical image analysis aids disease diagnosis .',\n",
       " 'explainable computer vision interprets image features .',\n",
       " 'ai-driven personalization enhances user experiences .',\n",
       " 'conversational ai improves human-like interactions .',\n",
       " 'federated learning protects individual data privacy .',\n",
       " 'human-centered ai design prioritizes user needs .',\n",
       " 'quantum annealing solves optimization problems .',\n",
       " 'behavior-based authentication enhances security .',\n",
       " 'ai music composition transforms creative processes .',\n",
       " 'semantic search improves information retrieval .',\n",
       " 'recommender systems optimize user preferences .',\n",
       " 'information retrieval systems outdated without neural networks']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# Tokenize documents\n",
    "tokenized_documents = [word_tokenize(document) for document in documents]\n",
    "\n",
    "# Define the list of English stopwords\n",
    "english_stopwords = stopwords.words('english')\n",
    "\n",
    "# Preprocess the tokenized documents: remove stopwords and convert to lowercase\n",
    "cleaned_data = [[word.lower() for word in document if word.lower() not in english_stopwords] for document in tokenized_documents]\n",
    "\n",
    "# Join tokens back into sentences\n",
    "cleaned_sentences = [' '.join(document) for document in cleaned_data]\n",
    "cleaned_sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: TF-IDF Calculation\n",
    "We convert our clean text data into TF-IDF vectors. This numerical transformation emphasizes the importance of words across documents and queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 163)\t0.49120372384880856\n",
      "  (0, 327)\t0.49120372384880856\n",
      "  (0, 312)\t0.49120372384880856\n",
      "  (0, 181)\t0.34109961564339564\n",
      "  (0, 186)\t0.3997596243233148\n",
      "  (1, 11)\t0.3259493911700577\n",
      "  (1, 305)\t0.43071911294080617\n",
      "  (1, 151)\t0.4693864999729862\n",
      "  (1, 242)\t0.4032841652344177\n",
      "  (1, 178)\t0.4032841652344177\n",
      "  (1, 202)\t0.4032841652344177\n",
      "  (2, 241)\t0.43630037870457955\n",
      "  (2, 188)\t0.4085099284682032\n",
      "  (2, 81)\t0.4085099284682032\n",
      "  (2, 158)\t0.47546881841107386\n",
      "  (2, 9)\t0.43630037870457955\n",
      "  (2, 7)\t0.24382911739385216\n",
      "  (3, 168)\t0.40332998980704693\n",
      "  (3, 123)\t0.4395385451021021\n",
      "  (3, 176)\t0.4395385451021021\n",
      "  (3, 273)\t0.4395385451021021\n",
      "  (3, 78)\t0.5076834854816823\n",
      "  (4, 289)\t0.4472135954999579\n",
      "  (4, 148)\t0.4472135954999579\n",
      "  (4, 56)\t0.4472135954999579\n",
      "  :\t:\n",
      "  (95, 23)\t0.4623932891322133\n",
      "  (95, 111)\t0.3619774719865355\n",
      "  (96, 71)\t0.4425900318406722\n",
      "  (96, 313)\t0.4425900318406722\n",
      "  (96, 61)\t0.4425900318406722\n",
      "  (96, 201)\t0.4425900318406722\n",
      "  (96, 241)\t0.40613009944220513\n",
      "  (96, 7)\t0.2269682735277233\n",
      "  (97, 268)\t0.4455030793792585\n",
      "  (97, 274)\t0.4854976825366838\n",
      "  (97, 279)\t0.4854976825366838\n",
      "  (97, 159)\t0.39511583841421116\n",
      "  (97, 165)\t0.4171264568459447\n",
      "  (98, 233)\t0.4976450584487141\n",
      "  (98, 213)\t0.4976450584487141\n",
      "  (98, 255)\t0.4976450584487141\n",
      "  (98, 324)\t0.37098218197927724\n",
      "  (98, 301)\t0.34557258001557645\n",
      "  (99, 336)\t0.4278457597149894\n",
      "  (99, 216)\t0.4278457597149894\n",
      "  (99, 268)\t0.39260043931926286\n",
      "  (99, 206)\t0.34819658701250344\n",
      "  (99, 207)\t0.3675934865311952\n",
      "  (99, 165)\t0.3675934865311952\n",
      "  (99, 301)\t0.29710284573974216\n"
     ]
    }
   ],
   "source": [
    "# Define the TF-IDF Vectorizer and transform the cleaned sentences\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(cleaned_sentences)\n",
    "\n",
    "# Show the TF-IDF matrix\n",
    "print(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 150)\t0.8898182575679641\n",
      "  (0, 7)\t0.45631509782025886\n",
      "  (1, 283)\t0.49120372384880856\n",
      "  (1, 186)\t0.3997596243233148\n",
      "  (1, 181)\t0.34109961564339564\n",
      "  (1, 158)\t0.49120372384880856\n",
      "  (1, 132)\t0.49120372384880856\n",
      "  (2, 328)\t0.675893887742558\n",
      "  (2, 28)\t0.6328423655134223\n",
      "  (2, 7)\t0.37772740557659634\n",
      "  (3, 242)\t0.5773502691896257\n",
      "  (3, 202)\t0.5773502691896257\n",
      "  (3, 178)\t0.5773502691896257\n",
      "  (4, 271)\t0.6647515987814472\n",
      "  (4, 33)\t0.6647515987814472\n",
      "  (4, 7)\t0.34089679352410995\n",
      "  (5, 100)\t0.7069016836152037\n",
      "  (5, 50)\t0.6073507768032713\n",
      "  (5, 7)\t0.3625121289259898\n",
      "  (6, 273)\t0.4893761419033552\n",
      "  (6, 230)\t0.44906203682282747\n",
      "  (6, 78)\t0.2826238883956818\n",
      "  (6, 54)\t0.4893761419033552\n",
      "  (6, 51)\t0.4893761419033552\n",
      "  (7, 116)\t0.524313122582309\n",
      "  (7, 86)\t0.559981527989344\n",
      "  (7, 68)\t0.559981527989344\n",
      "  (7, 7)\t0.3129490790998236\n",
      "  (8, 287)\t0.552778160814236\n",
      "  (8, 53)\t0.7415108690124079\n",
      "  (8, 7)\t0.3802603529994866\n",
      "  (9, 110)\t0.6647515987814472\n",
      "  (9, 77)\t0.6647515987814472\n",
      "  (9, 7)\t0.34089679352410995\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Tokenize, clean, and prepare queries similarly to documents\n",
    "tokenized_queries = [word_tokenize(query) for query in queries]\n",
    "cleaned_queries = [\n",
    "    ' '.join([word.lower() for word in query if word.lower() not in english_stopwords])\n",
    "    for query in tokenized_queries\n",
    "]\n",
    "\n",
    "# Transform the cleaned queries to TF-IDF vectors using the same vectorizer\n",
    "tfidf_queries = tfidf_vectorizer.transform(cleaned_queries)\n",
    "print(tfidf_queries)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Cosine Similarity Calculation\n",
    "With our documents as TF-IDF vectors, we now calculate the cosine similarity between each document and each query. This similarity score helps us gauge the relevance of documents to queries.\n",
    "before calculating the cosine similarity,I will preprocessing the queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What are the impacts of AI on healthcare?\n",
      "Cosine Similarities: [[0.         0.         0.11126291 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.12189804 0.57900347 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.10717419 0.\n",
      "  0.         0.1295451  0.         0.         0.         0.\n",
      "  0.         0.13511749 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.12831544 0.10839286 0.\n",
      "  0.         0.10873925 0.         0.         0.         0.10520966\n",
      "  0.         0.         0.         0.         0.         0.11544205\n",
      "  0.         0.         0.10749035 0.         0.11552197 0.\n",
      "  0.         0.         0.         0.         0.11126291 0.\n",
      "  0.         0.         0.11754444 0.         0.         0.\n",
      "  0.         0.         0.09814197 0.         0.         0.\n",
      "  0.         0.10633167 0.         0.         0.         0.\n",
      "  0.12232735 0.11193098 0.         0.10199151 0.         0.\n",
      "  0.10356905 0.         0.         0.        ]]\n",
      "Query: How does machine learning improve financial services?\n",
      "Cosine Similarities: [[0.27615671 0.         0.23355205 0.         0.         0.1012015\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.43934597\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.11529871 0.         0.\n",
      "  0.         0.13676784 0.25864524 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.15795136 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.23681683 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.1120485  0.         0.\n",
      "  0.         0.         0.11396038 0.         0.         0.\n",
      "  0.         0.         0.         0.        ]]\n",
      "Query: What is the role of AI in autonomous vehicles?\n",
      "Cosine Similarities: [[0.         0.         0.09210094 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.10090446 0.09979861 0.         0.61514135\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.08871639 0.\n",
      "  0.         0.10723453 0.         0.26466698 0.         0.\n",
      "  0.         0.11184722 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.10621664 0.08972518 0.\n",
      "  0.         0.09001191 0.         0.         0.         0.0870902\n",
      "  0.         0.         0.         0.         0.         0.09556034\n",
      "  0.         0.         0.0889781  0.         0.0956265  0.\n",
      "  0.         0.         0.         0.         0.09210094 0.\n",
      "  0.         0.         0.09730065 0.         0.         0.\n",
      "  0.         0.         0.08123972 0.         0.         0.\n",
      "  0.         0.08801897 0.57063803 0.         0.         0.\n",
      "  0.10125984 0.09265396 0.         0.08442629 0.         0.\n",
      "  0.08573214 0.         0.         0.        ]]\n",
      "Query: Explain the applications of natural language processing.\n",
      "Cosine Similarities: [[0.         0.69850866 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.22798389\n",
      "  0.         0.         0.22820781 0.         0.         0.23178076\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.24082807 0.         0.         0.         0.         0.\n",
      "  0.2237763  0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.20803976 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]]\n",
      "Query: How does robotics benefit from AI integration?\n",
      "Cosine Similarities: [[0.         0.         0.08312056 0.         0.29728595 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.09106569 0.09006767 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.38451972 0.\n",
      "  0.         0.09677854 0.         0.         0.         0.\n",
      "  0.         0.10094147 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.09585991 0.08097645 0.\n",
      "  0.         0.08123523 0.         0.         0.         0.0785984\n",
      "  0.         0.         0.         0.         0.         0.08624265\n",
      "  0.         0.         0.08030222 0.         0.08630236 0.\n",
      "  0.         0.         0.         0.         0.08312056 0.\n",
      "  0.         0.         0.08781327 0.         0.         0.\n",
      "  0.         0.         0.07331838 0.         0.         0.\n",
      "  0.         0.07943661 0.         0.         0.         0.\n",
      "  0.09138642 0.08361966 0.         0.07619424 0.         0.\n",
      "  0.07737276 0.         0.         0.        ]]\n",
      "Query: What are the challenges in implementing AI in education?\n",
      "Cosine Similarities: [[0.         0.         0.08839101 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.09683992 0.09577861 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.0851428  0.\n",
      "  0.         0.102915   0.         0.         0.         0.\n",
      "  0.         0.51551295 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.10193812 0.08611095 0.\n",
      "  0.         0.08638613 0.         0.         0.         0.08358211\n",
      "  0.         0.         0.         0.         0.         0.34913909\n",
      "  0.         0.         0.32509024 0.         0.09177456 0.\n",
      "  0.         0.         0.         0.         0.08839101 0.\n",
      "  0.         0.         0.09338127 0.         0.         0.\n",
      "  0.         0.         0.0779673  0.         0.         0.\n",
      "  0.         0.32158595 0.         0.         0.         0.\n",
      "  0.09718098 0.08892175 0.         0.0810255  0.         0.\n",
      "  0.08227875 0.         0.         0.        ]]\n",
      "Query: How does data science contribute to climate change predictions?\n",
      "Cosine Similarities: [[0.         0.         0.         0.35858316 0.         0.06973665\n",
      "  0.         0.         0.         0.         0.07286644 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.13696565 0.         0.         0.07152439 0.6369908  0.\n",
      "  0.         0.         0.07833189 0.         0.0825048  0.07501691\n",
      "  0.         0.         0.18957473 0.         0.         0.\n",
      "  0.         0.         0.08724758 0.         0.         0.\n",
      "  0.         0.         0.         0.07713108 0.         0.\n",
      "  0.         0.         0.07544942 0.         0.         0.\n",
      "  0.07363242 0.         0.         0.         0.         0.07501691\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.07420738\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.07852863 0.         0.         0.\n",
      "  0.         0.         0.         0.        ]]\n",
      "Query: What are the ethical considerations in AI development?\n",
      "Cosine Similarities: [[0.         0.         0.0763061  0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.85360746 0.08268366 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.07350198 0.\n",
      "  0.         0.08884435 0.         0.         0.         0.\n",
      "  0.         0.092666   0.         0.         0.         0.\n",
      "  0.         0.         0.         0.08800103 0.31235564 0.\n",
      "  0.         0.07457533 0.         0.         0.         0.07215467\n",
      "  0.         0.         0.         0.         0.         0.07917223\n",
      "  0.         0.         0.07371881 0.         0.07922704 0.\n",
      "  0.20689675 0.         0.         0.         0.0763061  0.\n",
      "  0.         0.         0.08061409 0.         0.         0.\n",
      "  0.         0.         0.06730752 0.         0.         0.\n",
      "  0.         0.07292416 0.47277651 0.         0.         0.\n",
      "  0.08389429 0.07676428 0.         0.06994761 0.         0.\n",
      "  0.07102951 0.         0.         0.        ]]\n",
      "Query: Explain the significance of AI in smart cities.\n",
      "Cosine Similarities: [[0.         0.         0.09271855 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.1015811  0.10046784 0.         0.\n",
      "  0.49653044 0.         0.         0.         0.19650261 0.\n",
      "  0.         0.         0.         0.         0.0893113  0.\n",
      "  0.         0.10795362 0.         0.         0.         0.\n",
      "  0.19650261 0.11259724 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.10692891 0.09032685 0.\n",
      "  0.         0.09061551 0.         0.         0.19491889 0.0876742\n",
      "  0.         0.         0.         0.18319136 0.         0.09620114\n",
      "  0.         0.         0.08957477 0.         0.09626775 0.\n",
      "  0.         0.         0.         0.         0.09271855 0.19306568\n",
      "  0.         0.         0.09795313 0.         0.         0.\n",
      "  0.         0.         0.0817845  0.         0.         0.\n",
      "  0.         0.0886092  0.         0.         0.         0.\n",
      "  0.10193886 0.09327527 0.         0.08499243 0.         0.\n",
      "  0.08630704 0.         0.         0.        ]]\n",
      "Query: How does AI enhance cybersecurity measures?\n",
      "Cosine Similarities: [[0.         0.         0.08312056 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.36338853 0.09106569 0.09006767 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.08006603 0.\n",
      "  0.         0.09677854 0.         0.         0.         0.\n",
      "  0.         0.10094147 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.09585991 0.08097645 0.\n",
      "  0.         0.08123523 0.         0.         0.         0.0785984\n",
      "  0.         0.         0.         0.29551542 0.         0.08624265\n",
      "  0.         0.         0.08030222 0.         0.08630236 0.\n",
      "  0.         0.         0.         0.         0.08312056 0.\n",
      "  0.         0.         0.08781327 0.         0.         0.\n",
      "  0.         0.         0.07331838 0.         0.         0.\n",
      "  0.         0.07943661 0.         0.         0.         0.\n",
      "  0.09138642 0.08361966 0.         0.07619424 0.         0.\n",
      "  0.07737276 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function to preprocess and transform a single query\n",
    "def preprocess_and_transform_query(query):\n",
    "    query_tokens = word_tokenize(query)\n",
    "    query_cleaned = [word.lower() for word in query_tokens if word.lower() not in english_stopwords]\n",
    "    query_cleaned_combined = ' '.join(query_cleaned)\n",
    "    query_tfIdf_vector = tfidf_vectorizer.transform([query_cleaned_combined])\n",
    "    return query_tfIdf_vector\n",
    "\n",
    "# Calculate cosine similarities for each query\n",
    "for query in queries:\n",
    "    query_tfIdf_vector = preprocess_and_transform_query(query)\n",
    "    cosine_similarities = cosine_similarity(query_tfIdf_vector, tfidf_matrix)\n",
    "    print(f\"Query: {query}\")\n",
    "    print(\"Cosine Similarities:\", cosine_similarities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking the document according to the cosine similarity\n",
    "Ranking top 10 for each query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1: What are the impacts of AI on healthcare?\n",
      "Top 10 documents ranked by cosine similarity:\n",
      "\t1. Document Index: 15 - Cosine Similarity: 0.579\n",
      "\t2. Document Index: 37 - Cosine Similarity: 0.135\n",
      "\t3. Document Index: 31 - Cosine Similarity: 0.130\n",
      "\t4. Document Index: 45 - Cosine Similarity: 0.128\n",
      "\t5. Document Index: 90 - Cosine Similarity: 0.122\n",
      "\t6. Document Index: 14 - Cosine Similarity: 0.122\n",
      "\t7. Document Index: 74 - Cosine Similarity: 0.118\n",
      "\t8. Document Index: 64 - Cosine Similarity: 0.116\n",
      "\t9. Document Index: 59 - Cosine Similarity: 0.115\n",
      "\t10. Document Index: 91 - Cosine Similarity: 0.112\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Query 2: How does machine learning improve financial services?\n",
      "Top 10 documents ranked by cosine similarity:\n",
      "\t1. Document Index: 23 - Cosine Similarity: 0.439\n",
      "\t2. Document Index: 0 - Cosine Similarity: 0.276\n",
      "\t3. Document Index: 38 - Cosine Similarity: 0.259\n",
      "\t4. Document Index: 69 - Cosine Similarity: 0.237\n",
      "\t5. Document Index: 2 - Cosine Similarity: 0.234\n",
      "\t6. Document Index: 54 - Cosine Similarity: 0.158\n",
      "\t7. Document Index: 37 - Cosine Similarity: 0.137\n",
      "\t8. Document Index: 33 - Cosine Similarity: 0.115\n",
      "\t9. Document Index: 92 - Cosine Similarity: 0.114\n",
      "\t10. Document Index: 87 - Cosine Similarity: 0.112\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Query 3: What is the role of AI in autonomous vehicles?\n",
      "Top 10 documents ranked by cosine similarity:\n",
      "\t1. Document Index: 17 - Cosine Similarity: 0.615\n",
      "\t2. Document Index: 86 - Cosine Similarity: 0.571\n",
      "\t3. Document Index: 33 - Cosine Similarity: 0.265\n",
      "\t4. Document Index: 37 - Cosine Similarity: 0.112\n",
      "\t5. Document Index: 31 - Cosine Similarity: 0.107\n",
      "\t6. Document Index: 45 - Cosine Similarity: 0.106\n",
      "\t7. Document Index: 90 - Cosine Similarity: 0.101\n",
      "\t8. Document Index: 14 - Cosine Similarity: 0.101\n",
      "\t9. Document Index: 15 - Cosine Similarity: 0.100\n",
      "\t10. Document Index: 74 - Cosine Similarity: 0.097\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Query 4: Explain the applications of natural language processing.\n",
      "Top 10 documents ranked by cosine similarity:\n",
      "\t1. Document Index: 1 - Cosine Similarity: 0.699\n",
      "\t2. Document Index: 54 - Cosine Similarity: 0.241\n",
      "\t3. Document Index: 41 - Cosine Similarity: 0.232\n",
      "\t4. Document Index: 38 - Cosine Similarity: 0.228\n",
      "\t5. Document Index: 35 - Cosine Similarity: 0.228\n",
      "\t6. Document Index: 60 - Cosine Similarity: 0.224\n",
      "\t7. Document Index: 80 - Cosine Similarity: 0.208\n",
      "\t8. Document Index: 27 - Cosine Similarity: 0.000\n",
      "\t9. Document Index: 28 - Cosine Similarity: 0.000\n",
      "\t10. Document Index: 29 - Cosine Similarity: 0.000\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Query 5: How does robotics benefit from AI integration?\n",
      "Top 10 documents ranked by cosine similarity:\n",
      "\t1. Document Index: 28 - Cosine Similarity: 0.385\n",
      "\t2. Document Index: 4 - Cosine Similarity: 0.297\n",
      "\t3. Document Index: 37 - Cosine Similarity: 0.101\n",
      "\t4. Document Index: 31 - Cosine Similarity: 0.097\n",
      "\t5. Document Index: 45 - Cosine Similarity: 0.096\n",
      "\t6. Document Index: 90 - Cosine Similarity: 0.091\n",
      "\t7. Document Index: 14 - Cosine Similarity: 0.091\n",
      "\t8. Document Index: 15 - Cosine Similarity: 0.090\n",
      "\t9. Document Index: 74 - Cosine Similarity: 0.088\n",
      "\t10. Document Index: 64 - Cosine Similarity: 0.086\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Query 6: What are the challenges in implementing AI in education?\n",
      "Top 10 documents ranked by cosine similarity:\n",
      "\t1. Document Index: 37 - Cosine Similarity: 0.516\n",
      "\t2. Document Index: 59 - Cosine Similarity: 0.349\n",
      "\t3. Document Index: 62 - Cosine Similarity: 0.325\n",
      "\t4. Document Index: 85 - Cosine Similarity: 0.322\n",
      "\t5. Document Index: 31 - Cosine Similarity: 0.103\n",
      "\t6. Document Index: 45 - Cosine Similarity: 0.102\n",
      "\t7. Document Index: 90 - Cosine Similarity: 0.097\n",
      "\t8. Document Index: 14 - Cosine Similarity: 0.097\n",
      "\t9. Document Index: 15 - Cosine Similarity: 0.096\n",
      "\t10. Document Index: 74 - Cosine Similarity: 0.093\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Query 7: How does data science contribute to climate change predictions?\n",
      "Top 10 documents ranked by cosine similarity:\n",
      "\t1. Document Index: 28 - Cosine Similarity: 0.637\n",
      "\t2. Document Index: 3 - Cosine Similarity: 0.359\n",
      "\t3. Document Index: 38 - Cosine Similarity: 0.190\n",
      "\t4. Document Index: 24 - Cosine Similarity: 0.137\n",
      "\t5. Document Index: 44 - Cosine Similarity: 0.087\n",
      "\t6. Document Index: 34 - Cosine Similarity: 0.083\n",
      "\t7. Document Index: 92 - Cosine Similarity: 0.079\n",
      "\t8. Document Index: 32 - Cosine Similarity: 0.078\n",
      "\t9. Document Index: 51 - Cosine Similarity: 0.077\n",
      "\t10. Document Index: 56 - Cosine Similarity: 0.075\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Query 8: What are the ethical considerations in AI development?\n",
      "Top 10 documents ranked by cosine similarity:\n",
      "\t1. Document Index: 14 - Cosine Similarity: 0.854\n",
      "\t2. Document Index: 86 - Cosine Similarity: 0.473\n",
      "\t3. Document Index: 46 - Cosine Similarity: 0.312\n",
      "\t4. Document Index: 66 - Cosine Similarity: 0.207\n",
      "\t5. Document Index: 37 - Cosine Similarity: 0.093\n",
      "\t6. Document Index: 31 - Cosine Similarity: 0.089\n",
      "\t7. Document Index: 45 - Cosine Similarity: 0.088\n",
      "\t8. Document Index: 90 - Cosine Similarity: 0.084\n",
      "\t9. Document Index: 15 - Cosine Similarity: 0.083\n",
      "\t10. Document Index: 74 - Cosine Similarity: 0.081\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Query 9: Explain the significance of AI in smart cities.\n",
      "Top 10 documents ranked by cosine similarity:\n",
      "\t1. Document Index: 18 - Cosine Similarity: 0.497\n",
      "\t2. Document Index: 36 - Cosine Similarity: 0.197\n",
      "\t3. Document Index: 22 - Cosine Similarity: 0.197\n",
      "\t4. Document Index: 52 - Cosine Similarity: 0.195\n",
      "\t5. Document Index: 71 - Cosine Similarity: 0.193\n",
      "\t6. Document Index: 57 - Cosine Similarity: 0.183\n",
      "\t7. Document Index: 37 - Cosine Similarity: 0.113\n",
      "\t8. Document Index: 31 - Cosine Similarity: 0.108\n",
      "\t9. Document Index: 45 - Cosine Similarity: 0.107\n",
      "\t10. Document Index: 90 - Cosine Similarity: 0.102\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Query 10: How does AI enhance cybersecurity measures?\n",
      "Top 10 documents ranked by cosine similarity:\n",
      "\t1. Document Index: 13 - Cosine Similarity: 0.363\n",
      "\t2. Document Index: 57 - Cosine Similarity: 0.296\n",
      "\t3. Document Index: 37 - Cosine Similarity: 0.101\n",
      "\t4. Document Index: 31 - Cosine Similarity: 0.097\n",
      "\t5. Document Index: 45 - Cosine Similarity: 0.096\n",
      "\t6. Document Index: 90 - Cosine Similarity: 0.091\n",
      "\t7. Document Index: 14 - Cosine Similarity: 0.091\n",
      "\t8. Document Index: 15 - Cosine Similarity: 0.090\n",
      "\t9. Document Index: 74 - Cosine Similarity: 0.088\n",
      "\t10. Document Index: 64 - Cosine Similarity: 0.086\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have a list 'queries' that contains the text of each query\n",
    "# Generate query names like \"Query 1\", \"Query 2\", ...\n",
    "query_names = [f\"Query {i+1}\" for i in range(len(queries))]\n",
    "\n",
    "# Now, modify the print statements to include query names\n",
    "for query_index, top_10_indices in enumerate(top_10_ranked_docs_per_query):\n",
    "    # Use the query name from the query_names list\n",
    "    query_name = query_names[query_index]\n",
    "    print(f\"{query_name}: {queries[query_index]}\")\n",
    "    print(\"Top 10 documents ranked by cosine similarity:\")\n",
    "    for rank, doc_index in enumerate(top_10_indices, start=1):\n",
    "        print(f\"\\t{rank}. Document Index: {doc_index} - Cosine Similarity: {cosine_similarities[query_index, doc_index]:.3f}\")\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation:<br>\n",
    "#### Precision at k (P@k) for k=5, k=6, and k=10 for each query.\n",
    "#### Mean Average Precision (MAP) across all queries. \n",
    "#### the Mean Reciprocal Rank (MRR) across all queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(relevant_indices, retrieved_indices, k):\n",
    "    relevant_set = set(relevant_indices)\n",
    "    retrieved_set = set(retrieved_indices[:k])\n",
    "    relevant_and_retrieved = len(relevant_set.intersection(retrieved_set))\n",
    "    return relevant_and_retrieved / k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_precision(relevant_indices, retrieved_indices):\n",
    "    ap_sum = 0\n",
    "    hit_count = 0\n",
    "    for i, doc_index in enumerate(retrieved_indices):\n",
    "        if doc_index in relevant_indices:\n",
    "            hit_count += 1\n",
    "            precision = hit_count / (i + 1)\n",
    "            ap_sum += precision\n",
    "    return ap_sum / len(relevant_indices) if relevant_indices else 0\n",
    "\n",
    "def mean_average_precision(relevant_indices_list, retrieved_indices_list):\n",
    "    aps = [average_precision(relevant, retrieved) for relevant, retrieved in zip(relevant_indices_list, retrieved_indices_list)]\n",
    "    return sum(aps) / len(aps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_reciprocal_rank(relevant_indices_list, retrieved_indices_list):\n",
    "    reciprocal_ranks = []\n",
    "    for relevant_indices, retrieved_indices in zip(relevant_indices_list, retrieved_indices_list):\n",
    "        for rank, doc_index in enumerate(retrieved_indices, start=1):\n",
    "            if doc_index in relevant_indices:\n",
    "                reciprocal_ranks.append(1 / rank)\n",
    "                break\n",
    "    return sum(reciprocal_ranks) / len(reciprocal_ranks) if reciprocal_ranks else 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average P@5: 0.120\n",
      "Average P@6: 0.117\n",
      "Average P@10: 0.100\n",
      "MAP: 0.038\n",
      "MRR: 0.370\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#here i transferred the details of the top 10 ranked documents per query to a list\n",
    "top_10_ranked_docs_per_query = [\n",
    "    [15, 37, 31, 45, 90, 14, 74, 64, 59, 91],\n",
    "\n",
    "     [23, 0, 38, 69, 2, 54, 37, 33, 92, 87],\n",
    "\n",
    "    [17, 86, 33, 37, 31, 45, 90, 14, 15, 74],\n",
    "\n",
    "    [1, 54, 41, 38, 35, 60, 80, 27, 28, 29],\n",
    "   \n",
    "    [28, 4, 37, 31, 45, 90, 14, 15, 74, 64],\n",
    "    \n",
    "    [37, 59, 62, 85, 31, 45, 90, 14, 15, 74],\n",
    "    \n",
    "    [28, 3, 38, 24, 44, 34, 92, 32, 51, 56],\n",
    "   \n",
    "    [14, 86, 46, 66, 37, 31, 45, 90, 15, 74],\n",
    "    \n",
    "    [18, 36, 22, 52, 71, 57, 37, 31, 45, 90],\n",
    "    \n",
    "    [13, 57, 37, 31, 45, 90, 14, 15, 74, 64]\n",
    "]\n",
    "\n",
    "# Calculate P@k for k=5, k=6, and k=10\n",
    "for k in [5, 6, 10]:\n",
    "    p_at_k_values = [precision_at_k(ground_truth[i], top_10_ranked_docs_per_query[i], k) for i in range(len(ground_truth))]\n",
    "    print(f\"Average P@{k}: {sum(p_at_k_values) / len(p_at_k_values):.3f}\")\n",
    "\n",
    "# Calculate MAP\n",
    "map_score = mean_average_precision(ground_truth, top_10_ranked_docs_per_query)\n",
    "print(f\"MAP: {map_score:.3f}\")\n",
    "\n",
    "# Calculate MRR\n",
    "mrr_score = mean_reciprocal_rank(ground_truth, top_10_ranked_docs_per_query)\n",
    "print(f\"MRR: {mrr_score:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Precision at k (P@k)**\n",
    "P@5: 0.120 - On average, 12% of the top 5 documents retrieved for each query are relevant.\n",
    "\n",
    "P@6: 0.117 - Extending to the top 6 documents, the precision slightly decreases to 11.7%, indicating that the additional documents at the 6th position are often not relevant.\n",
    "\n",
    "P@10: 0.100 - When considering the top 10 documents, the average precision further decreases to 10%. This suggests that as more documents are considered, the proportion of relevant documents within the top ranks decreases.\n",
    "\n",
    "**Mean Average Precision (MAP): 0.038**\n",
    "The MAP score of 0.038 is quite low, indicating that on average, the system does not rank relevant documents highly across all queries.\n",
    "\n",
    "**Mean Reciprocal Rank (MRR): 0.370**\n",
    "An MRR of 0.370 indicates that on average, the first relevant document appears relatively high in the ranked list across queries. The MRR score is This score is about how quickly we find the first good document, not how many good documents there are in total.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------\n",
    "- Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 is not relevant, 1 is relevant\n",
    "annotator1_relevance = [1, 0, 1, 0, 1, 1, 0, 0, 1, 0]  \n",
    "annotator2_relevance = [1, 1, 1, 0, 1, 0, 0, 1, 1, 1]  \n",
    "annotator3_relevance = [1, 0, 0, 0, 1, 0, 0, 1, 1, 0]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa (Annotator 1 vs. Annotator 2): 0.200\n",
      "Kappa (Annotator 1 vs. Annotator 3): 0.400\n",
      "Kappa (Annotator 2 vs. Annotator 3): 0.444\n"
     ]
    }
   ],
   "source": [
    "## TODO: Implement your solution for part 2\n",
    "\n",
    "\n",
    "\n",
    "# Compute pairwise Cohen's Kappa scores\n",
    "kappa_1_2 = cohen_kappa_score(annotator1_relevance, annotator2_relevance)\n",
    "kappa_1_3 = cohen_kappa_score(annotator1_relevance, annotator3_relevance)\n",
    "kappa_2_3 = cohen_kappa_score(annotator2_relevance, annotator3_relevance)\n",
    "\n",
    "print(f\"Kappa (Annotator 1 vs. Annotator 2): {kappa_1_2:.3f}\")\n",
    "print(f\"Kappa (Annotator 1 vs. Annotator 3): {kappa_1_3:.3f}\")\n",
    "print(f\"Kappa (Annotator 2 vs. Annotator 3): {kappa_2_3:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussing the Observed Agreement Levels**<br>\n",
    "● Kappa >0.8  good agreement\n",
    "●  0.67<Kappa<0.8  Tentative Conclusions\n",
    "●  >2 judges: average pairwise 𝑘𝑎𝑝𝑝𝑎\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Kappa (Annotator 1 vs. Annotator 2) = 0.200: This indicates slight agreement beyond chance between Annotator 1 and Annotator 2. The low score suggests that the criteria for judging relevance might not be consistently understood or applied between these two annotators.\n",
    "\n",
    "- Kappa (Annotator 1 vs. Annotator 3) = 0.400: This score falls into the \"fair agreement\" category, suggesting that while Annotator 1 and Annotator 3 have some level of consistent application of the relevance criteria, there's still a significant room for improvement.\n",
    "\n",
    "- Kappa (Annotator 2 vs. Annotator 3) = 0.444: This is close to moderate agreement but still indicates fair agreement. It suggests that Annotators 2 and 3 have a relatively more consistent understanding and application of the relevance criteria compared to the agreement between Annotator 1 and the others.\n",
    "\n",
    "the scores show that the people marking documents as relevant or not don't always agree with each other. This could be because the rules they're following aren't clear enough, or they have their own opinions influencing their choices."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSCN8010_classic_ml",
   "language": "python",
   "name": "cscn8010_classic_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
