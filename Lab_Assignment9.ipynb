{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Assignment 9\n",
    "#### Student Name:Fatimah Almusawi\n",
    "#### ID:9000400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sentences = [\n",
    "    \"Python is a versatile programming language.\",\n",
    "    \"JavaScript is widely used for web development.\",\n",
    "    \"Java is known for its platform independence.\",\n",
    "    \"Programming involves writing code to solve problems.\",\n",
    "    \"Data structures are crucial for efficient programming.\",\n",
    "    \"Algorithms are step-by-step instructions for solving problems.\",\n",
    "    \"Version control systems help manage code changes in collaboration.\",\n",
    "    \"Debugging is the process of finding and fixing errors in code.\",\n",
    "    \"Web frameworks simplify the development of web applications.\",\n",
    "    \"Artificial intelligence can be applied in various programming tasks.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the sample sentences above. You are required for this assignment to implement four functions **from scratch**. <br>\n",
    "You are required to preprocess the text and apply the tokenization process as done in assignment 8. (3)\n",
    "***THEN***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\marya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['python', 'versatile', 'programming', 'language']\n",
      "['javascript', 'widely', 'used', 'web', 'development']\n",
      "['java', 'known', 'platform', 'independence']\n",
      "['programming', 'involves', 'writing', 'code', 'solve', 'problem']\n",
      "['data', 'structure', 'crucial', 'efficient', 'programming']\n",
      "['algorithm', 'stepbystep', 'instruction', 'solving', 'problem']\n",
      "['version', 'control', 'system', 'help', 'manage', 'code', 'change', 'collaboration']\n",
      "['debugging', 'process', 'finding', 'fixing', 'error', 'code']\n",
      "['web', 'framework', 'simplify', 'development', 'web', 'application']\n",
      "['artificial', 'intelligence', 'applied', 'various', 'programming', 'task']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define stopwords\n",
    "stop_words = [\"is\", \"are\", \"the\", \"for\", \"a\", \"an\", \"of\", \"in\", \"and\", \"on\", \"to\", \"at\", \"its\", \"be\", \"can\", \"be\"]\n",
    "\n",
    "# Preprocess sentences\n",
    "cleaned_sentences = []  # This will now hold the final preprocessed sentences\n",
    "for sentence in sample_sentences:\n",
    "    # Lowercase and remove punctuation\n",
    "    sentence = sentence.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "    # Tokenize and preprocess (remove stopwords and lemmatize) in one step\n",
    "    cleaned_sentence = [lemmatizer.lemmatize(word) for word in sentence.split() if word not in stop_words]\n",
    "    # Update the cleaned_sentences list with the preprocessed sentence\n",
    "    cleaned_sentences.append(cleaned_sentence)\n",
    "\n",
    "# Print the cleaned_sentences\n",
    "for sentence in cleaned_sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "#### Part 1: Create a method that takes as an input a 2-dimensional list where each of the inner dimensions is a sentence list of tokens, and the outer dimension is the list of the sentences. The method MUST return the inverted index that is sufficient to represent the document. Assume that each sentence is a document and the sentence ID starts from 1. (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'python': [0], 'versatile': [0], 'programming': [0, 3, 4, 9], 'language': [0], 'javascript': [1], 'widely': [1], 'used': [1], 'web': [1, 8, 8], 'development': [1, 8], 'java': [2], 'known': [2], 'platform': [2], 'independence': [2], 'involves': [3], 'writing': [3], 'code': [3, 6, 7], 'solve': [3], 'problem': [3, 5], 'data': [4], 'structure': [4], 'crucial': [4], 'efficient': [4], 'algorithm': [5], 'stepbystep': [5], 'instruction': [5], 'solving': [5], 'version': [6], 'control': [6], 'system': [6], 'help': [6], 'manage': [6], 'change': [6], 'collaboration': [6], 'debugging': [7], 'process': [7], 'finding': [7], 'fixing': [7], 'error': [7], 'framework': [8], 'simplify': [8], 'application': [8], 'artificial': [9], 'intelligence': [9], 'applied': [9], 'various': [9], 'task': [9]}\n"
     ]
    }
   ],
   "source": [
    "def get_inverted_index(list_of_sentence_tokens):\n",
    "    ## TODO: Implement the functionality that will return the inverted index\n",
    "    ## for the list of sentence tokens\n",
    "    ## The input is a list of lists, where each list contains tokens of a sentence\n",
    "    ## The output should be a dictionary where the key is the token and the value is the list of sentence indices\n",
    "    ## in which the token appears\n",
    "    inverted_index = {}\n",
    "    for i, tokens in enumerate(list_of_sentence_tokens):\n",
    "        for token in tokens:\n",
    "            if token in inverted_index:\n",
    "                inverted_index[token].append(i)\n",
    "            else:\n",
    "                inverted_index[token] = [i]\n",
    "    return inverted_index\n",
    "# Print the inverted index\n",
    "print(get_inverted_index(cleaned_sentences))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "#### Part 2: Create a method that takes as an input a 2-dimensional list where each of the inner dimensions is a sentence list of tokens, and the outer dimension is the list of the sentences. The method MUST return the Positional index that is sufficient to represent the document. Assume that each sentence is a document and the sentence ID starts from 1, and the first token in the list is at position 0. Make sure to consider multiple appearance of the same token. (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'python': {1: [0]}, 'versatile': {1: [1]}, 'programming': {1: [2], 4: [0], 5: [4], 10: [4]}, 'language': {1: [3]}, 'javascript': {2: [0]}, 'widely': {2: [1]}, 'used': {2: [2]}, 'web': {2: [3], 9: [0, 4]}, 'development': {2: [4], 9: [3]}, 'java': {3: [0]}, 'known': {3: [1]}, 'platform': {3: [2]}, 'independence': {3: [3]}, 'involves': {4: [1]}, 'writing': {4: [2]}, 'code': {4: [3], 7: [5], 8: [5]}, 'solve': {4: [4]}, 'problem': {4: [5], 6: [4]}, 'data': {5: [0]}, 'structure': {5: [1]}, 'crucial': {5: [2]}, 'efficient': {5: [3]}, 'algorithm': {6: [0]}, 'stepbystep': {6: [1]}, 'instruction': {6: [2]}, 'solving': {6: [3]}, 'version': {7: [0]}, 'control': {7: [1]}, 'system': {7: [2]}, 'help': {7: [3]}, 'manage': {7: [4]}, 'change': {7: [6]}, 'collaboration': {7: [7]}, 'debugging': {8: [0]}, 'process': {8: [1]}, 'finding': {8: [2]}, 'fixing': {8: [3]}, 'error': {8: [4]}, 'framework': {9: [1]}, 'simplify': {9: [2]}, 'application': {9: [5]}, 'artificial': {10: [0]}, 'intelligence': {10: [1]}, 'applied': {10: [2]}, 'various': {10: [3]}, 'task': {10: [5]}}\n"
     ]
    }
   ],
   "source": [
    "def get_positional_index(list_of_sentence_tokens):\n",
    "    ## TODO: Implement the functionality that will return the positional index\n",
    "   \n",
    "    positional_index = {}\n",
    "    for i, tokens in enumerate(list_of_sentence_tokens,start=1):# here i added start=1 to start the sentence ID from 1\n",
    "        for j, token in enumerate(tokens):\n",
    "            if token in positional_index:\n",
    "                if i in positional_index[token]:\n",
    "                    positional_index[token][i].append(j)\n",
    "                else:\n",
    "                    positional_index[token][i] = [j]\n",
    "            else:\n",
    "                positional_index[token] = {i: [j]}\n",
    "    return positional_index\n",
    "# Print the positional index\n",
    "print(get_positional_index(cleaned_sentences))\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "#### Part 3: Create a method that takes as an input a 2-dimensional list where each of the inner dimensions is a sentence list of tokens, and the outer dimension is the list of the sentences. The method MUST return the TF-IDF Matrix that is sufficient to represent the documents. Assume that each sentence is a document and the sentence ID starts from 1. (7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1: {'python': 0.5756462732485115, 'versatile': 0.5756462732485115, 'programming': 0.22907268296853878, 'language': 0.5756462732485115}\n",
      "Document 2: {'javascript': 0.4605170185988092, 'widely': 0.4605170185988092, 'used': 0.4605170185988092, 'web': 0.3218875824868201, 'development': 0.3218875824868201}\n",
      "Document 3: {'java': 0.5756462732485115, 'known': 0.5756462732485115, 'platform': 0.5756462732485115, 'independence': 0.5756462732485115}\n",
      "Document 4: {'programming': 0.15271512197902584, 'involves': 0.3837641821656743, 'writing': 0.3837641821656743, 'code': 0.20066213405432268, 'solve': 0.3837641821656743, 'problem': 0.26823965207235}\n",
      "Document 5: {'data': 0.4605170185988092, 'structure': 0.4605170185988092, 'crucial': 0.4605170185988092, 'efficient': 0.4605170185988092, 'programming': 0.18325814637483104}\n",
      "Document 6: {'algorithm': 0.4605170185988092, 'stepbystep': 0.4605170185988092, 'instruction': 0.4605170185988092, 'solving': 0.4605170185988092, 'problem': 0.3218875824868201}\n",
      "Document 7: {'version': 0.28782313662425574, 'control': 0.28782313662425574, 'system': 0.28782313662425574, 'help': 0.28782313662425574, 'manage': 0.28782313662425574, 'code': 0.15049660054074201, 'change': 0.28782313662425574, 'collaboration': 0.28782313662425574}\n",
      "Document 8: {'debugging': 0.3837641821656743, 'process': 0.3837641821656743, 'finding': 0.3837641821656743, 'fixing': 0.3837641821656743, 'error': 0.3837641821656743, 'code': 0.20066213405432268}\n",
      "Document 9: {'web': 0.5364793041447, 'framework': 0.3837641821656743, 'simplify': 0.3837641821656743, 'development': 0.26823965207235, 'application': 0.3837641821656743}\n",
      "Document 10: {'artificial': 0.3837641821656743, 'intelligence': 0.3837641821656743, 'applied': 0.3837641821656743, 'various': 0.3837641821656743, 'programming': 0.15271512197902584, 'task': 0.3837641821656743}\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "def get_TFIDF_matrix(list_of_sentence_tokens):\n",
    "    \"\"\"\n",
    "   TF-IDF matrix for a list of tokenized sentences.\n",
    "    \n",
    "    Parameters:\n",
    "    - list_of_sentence_tokens: A 2-dimensional list where each inner list is a tokenized sentence.\n",
    "    \n",
    "    Returns:\n",
    "    - A list of dictionaries, where each dictionary represents a sentense with terms and their TF-IDF scores.\n",
    "    * in my case the sentence is the document\n",
    "    \"\"\"\n",
    "    # Initialize variables for IDF calculation\n",
    "    idf_counts = Counter()\n",
    "    num_documents = len(list_of_sentence_tokens)\n",
    "    \n",
    "    # Calculate document frequencies for IDF\n",
    "    for document in list_of_sentence_tokens:\n",
    "        unique_terms = set(document)\n",
    "        for term in unique_terms:\n",
    "            idf_counts[term] += 1\n",
    "    \n",
    "    # Calculate IDF for each term\n",
    "    idf = {term: math.log(num_documents / count) for term, count in idf_counts.items()}\n",
    "    \n",
    "    # Initialize the TF-IDF matrix\n",
    "    tf_idf_matrix = []\n",
    "    \n",
    "    # Calculate TF-IDF for each document\n",
    "    for document in list_of_sentence_tokens:\n",
    "        doc_tf = Counter(document)\n",
    "        doc_length = len(document)\n",
    "        doc_tf_idf = {term: (count / doc_length) * idf[term] for term, count in doc_tf.items()}\n",
    "        tf_idf_matrix.append(doc_tf_idf)\n",
    "    \n",
    "    return tf_idf_matrix\n",
    "\n",
    "\n",
    "tf_idf_matrix = get_TFIDF_matrix(cleaned_sentences)\n",
    "\n",
    "# Print the TF-IDF matrix\n",
    "for doc_index, tf_idf_scores in enumerate(tf_idf_matrix, start=1):\n",
    "    print(f\"Document {doc_index}: {tf_idf_scores}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High TF-IDF Scores: Words that are unique to a document (e.g., \"python\" in Document 1) have higher TF-IDF scores. This indicates that these words are very important in the context of their specific document because they appear in that document but not in many others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "#### Part 4: Create a method that takes as an input: (10)\n",
    " - a 2-dimensional list where each of the inner dimensions is a sentence list of tokens, and the outer dimension is the list of the sentences.\n",
    " - A method name: \"tfidf\", \"inverted\"\n",
    " - A Search Query\n",
    " - Return the rank of the sentences based on the given method and a query <br>\n",
    "\n",
    "***Hint: For inverted index we just want documents that have the query word/words, for tfidf you must show the ranking based on highest tfidf score***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranked documents: [0, 3, 4, 9, 1, 2, 5, 6, 7, 8]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "def get_ranked_documents(list_of_sentence_tokens, method_name, search_query):\n",
    "    query_tokens = search_query.lower().split()\n",
    "    doc_scores = []\n",
    "\n",
    "    if method_name == \"inverted\":\n",
    "        # Count the number of query tokens in each document\n",
    "        for document in list_of_sentence_tokens:\n",
    "            score = sum(token in query_tokens for token in document)\n",
    "            doc_scores.append(score)\n",
    "\n",
    "    elif method_name == \"tfidf\":\n",
    "        # Compute IDF for the entire corpus\n",
    "        num_documents = len(list_of_sentence_tokens)\n",
    "        all_tokens = [token for document in list_of_sentence_tokens for token in document]\n",
    "        idf_counts = Counter(set(all_tokens))\n",
    "        idf = {token: math.log(num_documents / (idf_counts[token] + 1)) for token in idf_counts}\n",
    "\n",
    "        # Compute TF-IDF score for each document based on the query\n",
    "        for document in list_of_sentence_tokens:\n",
    "            doc_tf = Counter(document)\n",
    "            tf_idf_score = sum((doc_tf[token] / len(document)) * idf.get(token, 0) for token in query_tokens)\n",
    "            doc_scores.append(tf_idf_score)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported method name. Choose either 'tfidf' or 'inverted'.\")\n",
    "\n",
    "    # Rank documents based on scores\n",
    "    if doc_scores:\n",
    "        # Pair each score with its document index\n",
    "        scored_indices = [(score, i) for i, score in enumerate(doc_scores)]\n",
    "        # Sort by score, then by original index\n",
    "        scored_indices.sort(key=lambda x: (-x[0], x[1]))\n",
    "        # Rebuild rank_list with sorted indices\n",
    "        rank_list = [index for _, index in scored_indices]\n",
    "\n",
    "    return rank_list\n",
    "\n",
    "\n",
    "# lets try the search query and method\n",
    "search_query = \"python programming\"\n",
    "method_name = \"tfidf\"  # we can Try \"inverted\" as well\n",
    "\n",
    "ranked_docs = get_ranked_documents(cleaned_sentences, method_name, search_query)\n",
    "print(\"Ranked documents:\", ranked_docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as we can see,the sentence one has the highest rank because we search with 2 words from sentence one(python programming)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSCN8010_classic_ml",
   "language": "python",
   "name": "cscn8010_classic_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
