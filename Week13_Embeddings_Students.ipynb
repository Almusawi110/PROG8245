{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 13 Word Embeddings Tutorial\n",
    "Tutorial for extracting word embeddings from words.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec embeddings using the Gensim library.<br> \n",
    "    Word2Vec is a popular technique for learning word embeddings, which are dense vector representations of words that capture semantic relationships between words based on their context.<br>\n",
    "    As discussed, Word2Vec have 2 types, Skipgrams and CBOW. Where SkipGrams are trained to predict context words given the target word, however CBOW is trained to predict target words given its context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Requirements: downloading punkt from nltk, and installing gensim library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing needed libraries\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sample Sentence\n",
    "text = \"\"\"\n",
    "Natural language processing (NLP) is a subfield of artificial intelligence (AI) that focuses on the interaction between computers and humans through natural language. NLP techniques aim to enable computers to understand, interpret, and generate human language in a way that is both meaningful and contextually relevant.\n",
    "\"\"\"\n",
    "tokenized_words = word_tokenize(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a model\n",
    "model_word2Vec = Word2Vec(sentences=[tokenized_words], vector_size=100, window=5,  min_count=1, workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get word vector for a specific word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Find similar words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SKIPGRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a model\n",
    "\n",
    "# Get word vector for a specific word\n",
    "\n",
    "# Find similar words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to save/load skipgram and CBOW models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to use word2Vec for SkipGrams and CBOW? Explore whether they will give different results for similar words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering Words based on Cooccurence Pattern --> Brown Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "import numpy as np\n",
    "# download brown corpus if not downloaded before\n",
    "# nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve sentences from brown corpus\n",
    "corpus = brown.sents()[:1]\n",
    "# transform all sentences to lower case\n",
    "corpus = [[word.lower() for word in sent] for sent in corpus]\n",
    "\n",
    "# Create a set of unique words in the corpus --> Vocab\n",
    "vocab = set(word for sent in corpus for word in sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find co-occurence pattern, co-occurence matrix is needed to show the word count and which words does it co-occur with\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing brown clustering, you need to set the number of clusters\n",
    "\n",
    "# assign each word as cluster\n",
    "\n",
    "# Perform Brown clustering by recursively merging clusters\n",
    "\n",
    "    # Find the pair of clusters with the highest co-occurrence count\n",
    " \n",
    "    # Merge the clusters by assigning the same cluster ID to both clusters\n",
    "\n",
    "    # Update the co-occurrence matrix by merging the counts of the two clusters\n",
    "\n",
    "# Final cluster assignments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: DONT RUN FOR LARGE DATASET**<br>\n",
    "Visualizing brown clusters: <br>\n",
    "Install scipy if you didn't use it before. pip install scipy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code should be able to visualize brown clustering, however you need to experiment it with less words as it does many\n",
    "# computations for the linkage matrix.\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "# Create a linkage matrix for hierarchical clustering\n",
    "\n",
    "# Plot the dendrogram\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe: \n",
    "Global Vectors for Word Representation is an unsupervised learning algorithm used to learn word embeddings from large amounts of text data. Word embeddings are dense vector representations of words that capture semantic relationships between words based on their co-occurrence statistics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Steps: Preprocess the text data.<br>\n",
    "Created the dictionary.<br>\n",
    "Traverse the glove file of a specific dimension and compare each word with all words in the dictionary,\n",
    "if a match occurs, copy the equivalent vector from the glove and paste into embedding_matrix at the corresponding index.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense vector for first word is =>  [-5.79900026e-01 -1.10100001e-01 -1.15569997e+00 -2.99059995e-03\n",
      " -2.06129998e-01  4.52890009e-01 -1.66710004e-01 -1.03820002e+00\n",
      " -9.92410004e-01  3.98840010e-01  5.92299998e-01  2.29900002e-01\n",
      "  1.52129996e+00 -1.77640006e-01 -2.97259986e-01 -3.92349988e-01\n",
      " -7.84709990e-01  1.55939996e-01  6.90769970e-01  5.95369995e-01\n",
      " -4.43399996e-01  5.35139978e-01  3.28530014e-01  1.24370003e+00\n",
      "  1.29719996e+00 -1.38779998e+00 -1.09249997e+00 -4.09249991e-01\n",
      " -5.69710016e-01 -3.46560001e-01  3.71630001e+00 -1.04890001e+00\n",
      " -4.67079997e-01 -4.47389990e-01  6.22999994e-03  1.96490008e-02\n",
      " -4.01609987e-01 -6.29130006e-01 -8.25060010e-01  4.55909997e-01\n",
      "  8.26259971e-01  5.70909977e-01  2.11989999e-01  4.68650013e-01\n",
      " -6.00269973e-01  2.99199998e-01  6.79440022e-01  1.42379999e+00\n",
      " -3.21520008e-02 -1.26029998e-01]\n"
     ]
    }
   ],
   "source": [
    "#Download Glove Pretrained Embeddings From: http://nlp.stanford.edu/data/glove.6B.zip  \n",
    "\n",
    "def embedding_for_vocab(filepath, word_index,\n",
    "                        embedding_dim):\n",
    "    vocab_size = len(word_index) + 1\n",
    "      \n",
    "    # Adding again 1 because of reserved 0 index\n",
    "    embedding_matrix_vocab = np.zeros((vocab_size,\n",
    "                                       embedding_dim))\n",
    "  \n",
    "    with open(filepath, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index.index(word)\n",
    "                embedding_matrix_vocab[idx] = np.array(\n",
    "                    vector, dtype=np.float32)[:embedding_dim]\n",
    "  \n",
    "    return embedding_matrix_vocab\n",
    "  \n",
    "  \n",
    "# matrix for vocab: tokenized_words\n",
    "embedding_dim = 50\n",
    "embedding_matrix_vocab = embedding_for_vocab(\n",
    "    './glove.6B/glove.6B.50d.txt', tokenized_words,\n",
    "  embedding_dim)\n",
    "  \n",
    "print(\"Dense vector for first word is => \",\n",
    "      embedding_matrix_vocab[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction --> SVD (LSA)\n",
    "    Latent Semantic Analysis (LSA) is a technique used in natural language processing to uncover the latent structure in a corpus of text documents by applying Singular Value Decomposition (SVD) to a term-document matrix. It allows us to reduce the dimensionality of the document-term space, thereby capturing the underlying semantic relationships between words and documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a text corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"The dog barked at the fox.\",\n",
    "    \"The fox ran away quickly.\",\n",
    "    \"The dog is lazy.\",\n",
    "    \"The fox is cunning.\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create Document-Term Matrix: We use the CountVectorizer from scikit-learn to convert the text documents into a document-term matrix. Each row in the matrix corresponds to a document, and each column represents a word's frequency in that document.\n",
    "\n",
    "- Apply LSA (SVD): We use the TruncatedSVD class from scikit-learn to perform Latent Semantic Analysis. We specify the number of components (dimensions) we want to reduce the feature space to (in this case, we use n_components=2 for simplicity).\n",
    "\n",
    "- Normalize Data: To ensure that each row in the transformed matrix has unit norm, we use the Normalizer from scikit-learn.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 14)\t2\n",
      "  (0, 11)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 5)\t1\n",
      "  (1, 14)\t2\n",
      "  (1, 6)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 0)\t1\n",
      "  (2, 14)\t1\n",
      "  (2, 6)\t1\n",
      "  (2, 13)\t1\n",
      "  (2, 1)\t1\n",
      "  (2, 12)\t1\n",
      "  (3, 14)\t1\n",
      "  (3, 9)\t1\n",
      "  (3, 5)\t1\n",
      "  (3, 7)\t1\n",
      "  (4, 14)\t1\n",
      "  (4, 6)\t1\n",
      "  (4, 7)\t1\n",
      "  (4, 4)\t1\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.9971702 , -1.12180185],\n",
       "       [ 2.43176445,  0.54765212],\n",
       "       [ 1.32104798,  1.45967715],\n",
       "       [ 1.4173243 , -0.50151648],\n",
       "       [ 1.32627769,  0.61297725]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply SVD (Latent Semantic Analysis)\n",
    "n_components = 2  # Number of components after reducing dimensions\n",
    "lsa = TruncatedSVD(n_components)\n",
    "X_lsa = lsa.fit_transform(X)\n",
    "X_lsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LSA Reduced Dimensionality:\n",
      "[[ 0.93654853 -0.35053794]\n",
      " [ 0.97556634  0.21970507]\n",
      " [ 0.67102161  0.7414378 ]\n",
      " [ 0.94272191 -0.33357967]\n",
      " [ 0.90773814  0.4195372 ]]\n"
     ]
    }
   ],
   "source": [
    "# Normalize the transformed data\n",
    "lsa_pipeline = make_pipeline(lsa, Normalizer(copy=False))\n",
    "X_lsa_normalized = lsa_pipeline.fit_transform(X)\n",
    "print(\"\\nLSA Reduced Dimensionality:\")\n",
    "print(X_lsa_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
